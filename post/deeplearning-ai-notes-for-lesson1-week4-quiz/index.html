<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Xavier&#39;s Blog</title>
<meta name="description" content="About code, about life, about everything." />
<link rel="shortcut icon" href="https://www.nxw.name/favicon.ico?v=1611342777069">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.2/animate.min.css">

<link rel="stylesheet" href="https://www.nxw.name/styles/main.css">


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-129706677-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-129706677-2');
</script>


  </head>
  <body>
    <div id="app" class="main px-4 flex flex-col lg:flex-row">
      <div id="sidebar" class="sidebar-wrapper lg:static lg:w-1/4">
  <div class="lg:sticky top-0">
    <div class="sidebar-content">
      <div class="flex lg:block p-4 lg:px-0 items-center fixed lg:static lg:block top-0 right-0 left-0 bg-white z-50">
        <i class="ri-menu-2-line lg:mt-4 text-2xl cursor-pointer animated fadeIn" onclick="openMenu()"></i>
        <a href="https://www.nxw.name">
          <img class="animated fadeInLeft avatar rounded-lg mx-4 lg:mt-32 lg:mx-0 mt-0 lg:w-24 lg:h-24 w-12 w-12" src="https://www.nxw.name/images/avatar.png?v=1611342777069" alt="">
        </a>
        <h1 class="animated fadeInLeft lg:text-4xl font-extrabold lg:mt-8 mt-0 text-xl" style="animation-delay: 0.2s">Xavier&#39;s Blog</h1>
      </div>
      
        <div class="animated fadeInLeft" style="animation-delay: 0.4s">
          <p class="my-4 text-gray-600 font-light hidden lg:block">
            Contents
          </p>
          <div class="toc-container hidden lg:block">
            <ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li><a href="#a-5-layer-neural-network">A 5-layer Neural Network</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      
    </div>
  </div>
</div>

<div class="menu-container">
  <i class="ri-arrow-left-line text-2xl cursor-pointer animated fadeIn close-menu-btn" onclick="closeMenu()"></i>
  <div>
    
      
        <a href="/" class="menu" style="animation-delay: 0s">
          Home
        </a>
      
    
      
        <a href="/archives" class="menu" style="animation-delay: 0.2s">
          Archives
        </a>
      
    
      
        <a href="/tags" class="menu" style="animation-delay: 0.4s">
          Tags
        </a>
      
    
      
        <a href="/ariang.html" class="menu" style="animation-delay: 0.6000000000000001s">
          AriaNG
        </a>
      
    
      
        <a href="/post/about" class="menu" style="animation-delay: 0.8s">
          About
        </a>
      
    
  </div>
  <div class="site-footer">
    <div class="py-4 text-gray-700">Powered by Gridea & GitHub.</div>
    <a class="rss" href="https://www.nxw.name/atom.xml" target="_blank">RSS</a>
  </div>
</div>
<div class="mask" onclick="closeMenu()">
</div>
      <div class="content-wrapper py-32 lg:p-8 lg:w-3/4 post-detail animated fadeIn">
        <h1 class="text-3xl font-bold lg:mt-16">DEEPLEARNING.AI NOTES FOR LESSON1_WEEK4 QUIZ</h1>
        <div class="text-sm text-gray-700 lg:my-8">
          2017-11-12 / 7 min read
        </div>
        
        <div class="post-content yue">
          <h3 id="a-5-layer-neural-network">A 5-layer Neural Network</h3>
<blockquote>
<p>Because it is very similar with 2-layer neural network, I only post the source code rather than  go through details with it, and some key points have been written into comments of code.</p>
</blockquote>
<!--more-->
<pre><code class="language-python">import numpy as np
    import h5py
    import matplotlib
    matplotlib.use('TkAgg')
    import matplotlib.pyplot as plt
    from deeplearning_ai_week4.testCases_v2 import *
    import deeplearning_ai_week4.dnn_app_utils_v2 as sup

    def initialize_parameters(n_x, n_h, n_y):
        W1 = np.random.randn(n_h, n_x) * 0.01
        b1 = np.zeros((n_h, 1))
        W2 = np.random.randn(n_y, n_h) * 0.01
        b2 = np.zeros((n_y, 1))

        assert (W1.shape == (n_h, n_x))
        assert (b1.shape == (n_h, 1))
        assert (W2.shape == (n_y, n_h))
        assert (b2.shape == (n_y, 1))

        parameters = {&quot;W1&quot;: W1,
                      &quot;b1&quot;: b1,
                      &quot;W2&quot;: W2,
                      &quot;b2&quot;: b2}

        return parameters

    def initialize_parameters_deep(layer_dims):
        np.random.seed(1)
        parameters = {}

        L = len(layer_dims)  # number of layers in the network

        for l in range(1, L):
            &quot;&quot;&quot;
            at very begin, I'm using np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01
            instead of np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l-1])
            but the results are very different
            so you can see that initializing parameters play a very important role in neural network
            &quot;&quot;&quot;
            parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l-1])
            parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))

            assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))
            assert (parameters['b' + str(l)].shape == (layer_dims[l], 1))

        return parameters

    def linear_forward(A, W, b):
        Z = np.dot(W, A) + b
        assert (Z.shape == (W.shape[0], A.shape[1]))
        cache = (A, W, b)

        return Z, cache

    def linear_activation_forward(A_prev, W, b, activation):
        if activation == &quot;sigmoid&quot;:
            Z, linear_cache = linear_forward(A_prev, W, b)
            A, activation_cache = sup.sigmoid(Z)
        elif activation == &quot;relu&quot;:
            Z, linear_cache = linear_forward(A_prev, W, b)
            A, activation_cache = sup.relu(Z)

        assert (A.shape == (W.shape[0], A_prev.shape[1]))
        cache = (linear_cache, activation_cache)

        return A, cache

    def L_model_forward(X, parameters):
        &quot;&quot;&quot;
        Arguments:
        X -- data, numpy array of shape (input size, number of examples)
        parameters -- output of initialize_parameters_deep()
        &quot;&quot;&quot;
        caches = []
        A = X
        L = len(parameters) // 2

        for l in range(1, L):
            A_prev = A
            A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)],
                                                 parameters['b' + str(l)], &quot;relu&quot;)
            caches.append(cache)

        AL, cache = linear_activation_forward(A, parameters['W' + str(L)],
                                              parameters['b' + str(L)], &quot;sigmoid&quot;)
        caches.append(cache)

        assert (AL.shape == (1, X.shape[1]))

        return AL, caches

    def compute_cost(AL, Y):
        m = Y.shape[1]

        cost = np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1 - Y), np.log(1 - AL))) / (-m)

        cost = np.squeeze(cost)
        assert cost.shape == ()

        return cost

    def linear_backward(dZ, cache):
        A_prev, W, b = cache
        m = A_prev.shape[1]

        dW = np.dot(dZ, A_prev.T) / m
        db = np.sum(dZ, axis=1, keepdims=True) / m
        dA_prev = np.dot(W.T, dZ)

        assert (dA_prev.shape == A_prev.shape)
        assert (dW.shape == W.shape)
        assert (db.shape == b.shape)

        return dA_prev, dW, db

    def linear_activation_backward(dA, cache, activation):
        linear_cache, activation_cache = cache

        if activation == &quot;relu&quot;:
            dZ = sup.relu_backward(dA, activation_cache)
        elif activation == &quot;sigmoid&quot;:
            dZ = sup.sigmoid_backward(dA, activation_cache)

        dA_prev, dW, db = linear_backward(dZ, linear_cache)

        return dA_prev, dW, db

    def L_model_backward(AL, Y, caches):
        &quot;&quot;&quot;
        Implement the backward propagation for the [LINEAR-&amp;gt;RELU] * (L-1) -&amp;gt; LINEAR -&amp;gt; SIGMOID group

        :param AL: probability vector, output of the forward propagation (L_model_forward())
        :param Y: true &quot;label&quot; vector (containing 0 if not-cat, 1 if cat)
        :param caches: list of caches containing:
                        every cache of linear_activation_forward() with &quot;relu&quot;
                            (it's caches[1], for 1 in range(L-1) i.e l = 0..L
                        the cache of linear_activation_forward() with &quot;sigmoid&quot; (it's caches[L-1])
        :return: A dictionary with gradients
                    grads[&quot;dA&quot; + str(l)] = ...
                    grads[&quot;dW&quot; + str(l)] = ...
                    grads[&quot;db&quot; + str(l)] = ...
        &quot;&quot;&quot;
        grads = {}
        L = len(caches)  # the number of layers
        m = AL.shape[1]  # the number of examples
        Y = Y.reshape(AL.shape)

        # Initializing the back propagation
        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

        # Lth layer (SIGMOID -&amp;gt; LINEAR) gradients.
        # Inputs: &quot;AL, Y, caches&quot;. Outputs: &quot;grads[&quot;dAL&quot;], grads[&quot;dWL&quot;], grads[&quot;dbL&quot;]
        current_cache = caches[L - 1]
        grads[&quot;dA&quot; + str(L)], grads[&quot;dW&quot; + str(L)], grads[&quot;db&quot; + str(L)] = \
            linear_activation_backward(dAL, current_cache, &quot;sigmoid&quot;)

        # other layers
        # range(L - 1): 1, 2, 3, ..., L-2 (NOT INCLUDING L - 1)
        for l in reversed(range(L - 1)):
            current_cache = caches[l]
            dA_prev_temp, dW_temp, db_temp = linear_activation_backward(
                grads[&quot;dA&quot; + str(l + 2)], current_cache, &quot;relu&quot;)
            # grads[&quot;dA&quot; + str(a)] refers to the output of the backward propagation
            # for example: for lth layer, the input is dA(l+1) and the output is dA(l)
            grads[&quot;dA&quot; + str(l + 1)] = dA_prev_temp
            grads[&quot;dW&quot; + str(l + 1)] = dW_temp
            grads[&quot;db&quot; + str(l + 1)] = db_temp

        return grads

    def update_parameters(parameters, grads, learning_rate):
        L = len(parameters) // 2

        for l in range(L):
            parameters[&quot;W&quot; + str(l + 1)] = parameters[&quot;W&quot; + str(l + 1)] - learning_rate * grads[&quot;dW&quot; + str(l + 1)]
            parameters[&quot;b&quot; + str(l + 1)] = parameters[&quot;b&quot; + str(l + 1)] - learning_rate * grads[&quot;db&quot; + str(l + 1)]

        return parameters

    def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):
        np.random.seed(1)
        costs = []

        parameters = initialize_parameters_deep(layers_dims)

        for i in range(0, num_iterations):
            AL, caches = L_model_forward(X, parameters)
            cost = compute_cost(AL, Y)
            grads = L_model_backward(AL, Y, caches)
            parameters = update_parameters(parameters, grads, learning_rate)
            costs.append(cost)
            if print_cost and i % 100 == 0:
                print(&quot;Cost after iteration %i: %f&quot; %(i, cost))

        plt.plot(np.squeeze(costs))
        plt.ylabel('cost')
        plt.xlabel('iterations (per tens)')
        plt.title(&quot;Learning rate =&quot; + str(learning_rate))
        plt.show()

        return parameters

    if __name__ == &quot;__main__&quot;:
        # matplotlib inline
        plt.rcParams['figure.figsize'] = (5., 4.)
        plt.rcParams['image.interpolation'] = 'nearest'
        plt.rcParams['image.cmap'] = 'gray'

        np.random.seed(1)

        train_x_orig, train_y, test_x_orig, test_y, classes = sup.load_data()

        m_train = train_x_orig.shape[0]
        num_px = train_x_orig.shape[1]
        m_test = test_x_orig.shape[0]

        train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T
        test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T

        train_x = train_x_flatten / 255.
        test_x = test_x_flatten / 255.

        # L-layer neural network core part
        layers_dims = [train_x.shape[0], 20, 7, 5, 1]   # 5-layer model
        parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations=2500, print_cost=True)

        pred_train = sup.predict(train_x, train_y, parameters)
</code></pre>

        </div>

        
          <a class="animated fadeInUp p-2 items-center text-sm text-gray-700 border hover:bg-gray-300 leading-none rounded-full flex lg:inline-flex m-4 " href="https://www.nxw.name/tag/2el9G3bDir/">
            <span class="flex-auto">deep learning</span>
          </a>
        


        <div class="flex justify-between py-8">
          
            <div class="prev-post">
              <a href="https://www.nxw.name/post/使用xpath匹配页面元素/">
                <h3 class="post-title">
                  <i class="ri-arrow-left-line"></i>
                  Scrapy 使用xpath匹配页面元素
                </h3>
              </a>
            </div>
          

          
            <div class="next-post">
              <a href="https://www.nxw.name/post/deeplearning-ai-notes-for-lesson1-week3-quiz/">
                <h3 class="post-title">
                  DEEPLEARNING.AI NOTES FOR LESSON1_WEEK3 QUIZ
                  <i class="ri-arrow-right-line"></i>
                </h3>
              </a>
            </div>
          
        </div>

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://geektutu.github.io/hexo-theme-geektutu/js/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: '95da5e6dfbe94e9621c5',
    clientSecret: '6b6510006d4bd9b042323bc2c5b1374561942033',
    repo: 'xav1er.github.io',
    owner: 'xavier-niu',
    admin: ['xavier-niu'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false,  // Facebook-like distraction free mode
    language: 'en'
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

      </div>
    </div>

    <script src="https://www.nxw.name/media/prism.js"></script>  
<script>

Prism.highlightAll()
let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

// This should probably be throttled.
// Especially because it triggers during smooth scrolling.
// https://lodash.com/docs/4.17.10#throttle
// You could do like...
// window.addEventListener("scroll", () => {
//    _.throttle(doThatStuff, 100);
// });
// Only not doing it here to keep this Pen dependency-free.

window.addEventListener("scroll", event => {
  let fromTop = window.scrollY;

  mainNavLinks.forEach((link, index) => {
    let section = document.getElementById(decodeURI(link.hash).substring(1));
    let nextSection = null
    if (mainNavLinks[index + 1]) {
      nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
    }
    if (section.offsetTop <= fromTop) {
      if (nextSection) {
        if (nextSection.offsetTop > fromTop) {
          link.classList.add("current");
        } else {
          link.classList.remove("current");    
        }
      } else {
        link.classList.add("current");
      }
    } else {
      link.classList.remove("current");
    }
  });
});


document.addEventListener("DOMContentLoaded", function() {
  var lazyImages = [].slice.call(document.querySelectorAll(".post-feature-image.lazy"));

  if ("IntersectionObserver" in window) {
    let lazyImageObserver = new IntersectionObserver(function(entries, observer) {
      entries.forEach(function(entry) {
        if (entry.isIntersecting) {
          let lazyImage = entry.target
          lazyImage.style.backgroundImage = `url(${lazyImage.dataset.bg})`
          lazyImage.classList.remove("lazy")
          lazyImageObserver.unobserve(lazyImage)
        }
      });
    });

    lazyImages.forEach(function(lazyImage) {
      lazyImageObserver.observe(lazyImage)
    })
  } else {
    // Possibly fall back to a more compatible method here
  }
});

const menuContainer = document.querySelector('.menu-container')
const menus = document.querySelectorAll('.menu-container .menu')
const mask = document.querySelector('.mask')
const contentWrapper = document.querySelector('.content-wrapper')
const latestArticle = document.querySelector('.latest-article')
const readMore = document.querySelector('.read-more')
const indexPage = document.querySelector('.index-page')

const isHome = location.pathname === '/'
if (latestArticle) {
  latestArticle.style.display = isHome ? 'block' : 'none'
  readMore.style.display = isHome ? 'block' : 'none'
  indexPage.style.display = isHome ? 'none' : 'block'
}

const openMenu = () => {
  menuContainer.classList.add('open')
  menus.forEach(menu => {
    menu.classList.add('animated', 'fadeInLeft')
  })
  mask.classList.add('open')
  contentWrapper.classList.add('is-second')
}

const closeMenu = () => {
  menuContainer.classList.remove('open')
  menus.forEach(menu => {
    menu.classList.remove('animated', 'fadeInLeft')
  })
  mask.classList.remove('open')
  contentWrapper.classList.remove('is-second')
}
</script>
  
  </body>
</html>
